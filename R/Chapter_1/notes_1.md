## Notes from session #2
### Pages 1-13


#### Statistical golems

Golems (mythical) are thoughtless 'machines' that do as they are told. Statistical tests are similar, they can't qualify if what they are doing is appropriate - they simply do. There are hundreds of these tests, some of which are practically never used appropriately. Each test has a unique set of assumptions and interpretations. To ensure appropriate use, one must be an expert of the domain of research and statistics, because one can miss the complexities of the other.

#### Statistical Rethinking

A lot of people view science as a pursuit of falsifying hypotheses. When using this logic of falsification or 'modus tollens' (method of destruction), you cannot be certain that you have come to any conclusion. This need not be interpretted as "Null hypothesis significance testing is bad", because that approach generally attempts to falsify a claim against the hypthesis, rather than the hypothesis itself.

The scientific method cannot be reduced to a statistical procedure, so our statistical methods should not pretend. This stems from that fact that there can be several hypotheses for a question. From these hypotheses, you have procedural/operational 'musts'. We then, even though it's not necessarily correct, attempt to falsify models that reflect some or all aspects of those 'musts'. However models can belong to multiple hyptheses. Rejecting a counterclaim does not without-a-doubt prove your hypothesis, nor does it specifically point to yours, not does it rule out unthought-of hypotheses.

Seeking falsifying evidence is an imperfect practice, for many reasons. For example, you rarely seek 1 data point that is a falsifier, and a falsifier or a "true-ifier" can be a measurement/observation error. It also becomes more difficult to falsify a probability or distribution - and other non discrete situations.

Falsification is practically never the result of a falsifier data point. It is the result of conversation and consensus that generally has to acknowledge measurement/observation error and/or the acknowledgement some non discrete reality to the problem. However this is not the history that is promoted, and it harms science internally and externally.

#### Tools

The worth-learning statistical tools of the social and biological sciences.

##### Bayesian Data Analysis

'Bayesian' tends to refer to a particular interpretation of "probability". Where the 'Frequentist' interprets probability as factual long-term proportion, the 'Bayesian' interprets it as a plausibiity, which strays from 0 and 1 due only to lack of information - "nothing is actually random". This is often how people actually think of a probability; this is exaggerated by the fact that people interpret 'non-Bayesian' probabilities as 'Bayesian', yet the reverse is unheard-of.

Bayesian statistical inference was popular, historically, first, however it signifcantly died in the 1900's due to popular opinion, but it has been on the rise since the 1990's.


